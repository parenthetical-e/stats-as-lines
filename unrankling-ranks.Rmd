---
title: "Statistical tests as linear models - unrankling ranks"
output: html_notebook
---

```{r, message=FALSE, echo=FALSE}
library(ggplot2)
library(tidyr)
library(dplyr)
library(broom)
library(MASS)
library(gridExtra)
library(grid)

signed_rank = function(x) sign(x) * rank(abs(x))

normalize <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
  }
```
# The premise
Jonas LindelÃ¸v wrote a [nice blog post](https://lindeloev.github.io/tests-as-linear/#3_pearson_and_spearman_correlation) explaining how common statistical tests amount to special cases of linear regression. This is real neat unfication of tests that at first pass look unrelated. His work included examples of nonparametric tests, based on ranks. But he seems to dismiss nonparametric tests as not that different from parametric ones. That seemed off. But I realized that my intuition for what ranks do to datawas too limited to say for sure. So I made some examples here to try and understand ranks better.


# Verification
## Parametric

First, I just want to try a couple examples of parametric tests from Jonas' post to make sure things check out.

### Correlation (Pearsons)
We'll need 20 samples for the equivilance to be exact.

```{r, message=FALSE, echo=TRUE, fig.width=0.9, fig.height=0.8}
# Create some data that covaries
covar <- matrix(c(1, 0.8, 1, 0.8), ncol=2)
M <- mvrnorm(n = 20, rep(0, 2), Sigma=covar)
X <- M[,1]
Y <- M[,2]

M %>% 
  as.data.frame() %>% 
  ggplot(aes(x=V1, y=V2)) +
  geom_point() + 
  theme_classic() +
  labs(x="X", y="Y")

rm(M, covar)
```

```{r, message=FALSE, echo=TRUE}
# Built in Pearons
a <- cor.test(X,Y)
at <- tidy(a)

# Equivalent linear model
b <- lm(Y ~ 1 + X)
bt = tidy(b)[2, ]  # Only slope
bt$conf.low = confint(b)[2,1]
bt$conf.high = confint(b)[2,2]
bt$method <- "lm(y ~ 1 + x)"

print(full_join(at, bt))
```

### _t_-test (one variable)
We'll need 20 samples for the equivilance to be exact.

```{r, message=FALSE, echo=FALSE, fig.width=0.6, fig.height=1.2}
Y1 <- rnorm(30, mean=1, sd=1)

df <- data.frame(data=Y1, name=rep("Y1", length(Y1))) 
df %>% 
  ggplot(aes(x=name, y=data)) + 
  geom_point() +
  theme_classic() +
  labs(x="Variable", y="data")
```

```{r, message=FALSE, echo=TRUE}
# Built-in t-test
a = t.test(Y1)
at <- tidy(a)
at$df = a$parameter

# Equivalent linear model
b = lm(Y1 ~ 1)
bt <- tidy(b)
bt$conf.low = confint(b)[1]
bt$conf.high = confint(b)[2]
bt$df = b$df.residual
bt$method <- "lm(y ~ 1)"

print(full_join(at, bt))
```

### _t_-test (two variable)
We'll need 20 samples for the equivilance to be exact.

```{r, message=FALSE, echo=TRUE, fig.width=0.6, fig.height=1.2}
Y0 <- rnorm(30, mean=0, sd=1)
Y1 <- rnorm(30, mean=1, sd=1)

df0 <- data.frame(data=Y0, name=rep("Y0", length(Y0)), N=1:length(Y0)) 
df1 <- data.frame(data=Y1, name=rep("Y1", length(Y1)), N=1:length(Y1)) 

df <- full_join(df0, df1)
df %>% 
  ggplot(aes(x=name, y=data, group=N)) + 
  geom_point() +
  # geom_line(size=0.5) +
  theme_classic() +
  labs(x="Variable", y="data")

rm(df0, df1)
```

```{r, message=FALSE, echo=FALSE, fig.width=0.6, fig.height=1.2}
# Built-in t-test
a = t.test(Y0, Y1, var.equal = TRUE)
at <- tidy(a)

# Equivalent linear model
Y = c(Y0, Y1)
group = rep(c('Y0', 'Y1'), each = 30)
b = lm(Y ~ 1 + I(group=='Y0'))
bt <- tidy(b)
bt$method <- "Y ~ 1 + I(group=='Y0')"

tmp = at$conf.high
at$conf.high = -at$conf.low
at$conf.low = -tmp
at$estimate2 = at$estimate2 - at$estimate1  # Make it the difference
at$df = at$parameter

# lm
bt$conf.low = confint(b)[2,1]
bt$conf.high = confint(b)[2,2]
bt$estimate1 = bt$estimate[1]
bt$estimate2 = bt$estimate[2]
bt$df = b$df.residual
print(full_join(at, bt[2,]))
```

## Nonparametric
Now let's try their nonparametric equivilants.

They, umm, don't seem as exact as the post suggests. I don't _think_ I've done something wrong.

### Correlation (Pearsons)

```{r, message=FALSE, echo=TRUE, fig.width=0.9, fig.height=0.8}
# Create some data that covaries
covar <- matrix(c(1, 0.8, 1, 0.8), ncol=2)
M <- mvrnorm(n = 20, rep(0, 2), Sigma=covar)
X <- M[,1]
Y <- M[,2]

# Built in Pearons
a <- cor.test(X, Y, method="spearman")
at <- tidy(a)

# Equivalent linear model
b <- lm(rank(Y) ~ 1 + rank(X))
bt = tidy(b)[2, ]  # Only slope
# bt$conf.low = confint(b)[2,1]
# bt$conf.high = confint(b)[2,2]
bt$method <- "lm(rank(y) ~ 1 + rank(x))"

print(full_join(at, bt))
rm(covar, M)
```

### _t_-test (one variable)
We'll need 20 samples for the equivilance to be exact.

```{r, message=FALSE, echo=FALSE, fig.width=0.6, fig.height=1.2}
Y1 <- rnorm(50, mean=1, sd=1)

# Built-in t-test
a = wilcox.test(Y1) 
at <- tidy(a)
# at$df = a$parameter

# Equivalent linear model
b = lm(signed_rank(Y1) ~ 1)
bt <- tidy(b)
bt$method <- "lm(signed_rank(Y1) ~ 1)"

print(full_join(at, bt))
```

### _t_-test (two variable)
We'll need 20 samples for the equivilance to be exact.

```{r, message=FALSE, echo=TRUE, fig.width=0.6, fig.height=1.2}
Y0 <- rnorm(30, mean=0, sd=1)
Y1 <- rnorm(30, mean=1, sd=1)

# Built-in t-test
a = wilcox.test(Y1, Y0)
at <- tidy(a)

# Equivalent linear model
Y = c(Y0, Y1)
group = rep(c('Y0', 'Y1'), each = 30)
b = lm(rank(Y) ~ 1 + I(group=='Y0'))
bt <- tidy(b)
bt$method <- "rank(Y) ~ 1 + I(group=='Y0')"

print(full_join(at, bt[2,]))
```



# Ranks 
What does ranking do to data? 

To find out let's first try some different noise distributions, then some outliers, and finally some linear and nonlinear series.

## Gaussian noise
```{r, message=FALSE, echo=FALSE, fig.width=0.8, fig.height=0.8}
Y1 <- rnorm(50, mean=1, sd=1)
ggplot(data.frame(Y1), aes(Y1)) + geom_histogram() + theme_classic()
```

```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=1.2}
df0 <- data.frame(data=normalize(Y1), name=rep("Y1", length(Y1)), N=1:length(Y1)) 
df1 <- data.frame(data=normalize(rank(Y1)), name=rep("rank(Y1)", length(Y1)), N=1:length(Y1)) 
df2 <- data.frame(data=normalize(signed_rank(Y1)), name=rep("signed_rank(Y1)", length(Y1)), N=1:length(Y1)) 
df <- full_join(df0, df1)
df <- full_join(df, df2)
df$name <- factor(df$name, levels=c("Y1", "signed_rank(Y1)", "rank(Y1)"))
df %>% 
  ggplot(aes(x=name, y=data, group=N)) + 
  geom_point() +
  geom_line(size=0.5, alpha=0.5) +
  theme_classic() +
  labs(x="Variable", y="Norm. data")
```

## Exponential noise
```{r, message=FALSE, echo=FALSE, fig.width=0.8, fig.height=0.8}
Y1 <- rexp(50, rate=1)
ggplot(data.frame(Y1), aes(Y1)) + geom_histogram() + theme_classic()
```

```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=1.2}
df0 <- data.frame(data=normalize(Y1), name=rep("Y1", length(Y1)), N=1:length(Y1)) 
df1 <- data.frame(data=normalize(rank(Y1)), name=rep("rank(Y1)", length(Y1)), N=1:length(Y1)) 
df2 <- data.frame(data=normalize(signed_rank(Y1)), name=rep("signed_rank(Y1)", length(Y1)), N=1:length(Y1)) 
df <- full_join(df0, df1)
df <- full_join(df, df2)
df$name <- factor(df$name, levels=c("Y1", "signed_rank(Y1)", "rank(Y1)"))
df %>% 
  ggplot(aes(x=name, y=data, group=N)) + 
  geom_point() +
  geom_line(size=0.5, alpha=0.5) +
  theme_classic() +
  labs(x="Variable", y="Norm. data")
```
## Log normal noise
```{r, message=FALSE, echo=FALSE, fig.width=0.8, fig.height=0.8}
Y1 <- rlnorm(50, meanlog = 1, sdlog = 1) - 6
ggplot(data.frame(Y1), aes(Y1)) + geom_histogram() + theme_classic()
```

```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=1.2}
df0 <- data.frame(data=normalize(Y1), name=rep("Y1", length(Y1)), N=1:length(Y1)) 
df1 <- data.frame(data=normalize(rank(Y1)), name=rep("rank(Y1)", length(Y1)), N=1:length(Y1)) 
df2 <- data.frame(data=normalize(signed_rank(Y1)), name=rep("signed_rank(Y1)", length(Y1)), N=1:length(Y1)) 
df <- full_join(df0, df1)
df <- full_join(df, df2)
df$name <- factor(df$name, levels=c("Y1", "signed_rank(Y1)", "rank(Y1)"))
df %>% 
  ggplot(aes(x=name, y=data, group=N)) + 
  geom_point() +
  geom_line(size=0.5, alpha=0.5) +
  theme_classic() +
  labs(x="Variable", y="Norm. data")
```

## Linear data

### Identical noise
```{r, message=FALSE, echo=FALSE, fig.width=1.4, fig.height=2}
N <- 30

m <- 0.1
b <- 0
X <- 1:N*m + b + rnorm(N, mean=0, sd=1)

m <- 0.6
b <- 0
Y <- 1:N*m + b + rnorm(N, mean=0, sd=1)

df <- data.frame(X=X, Y=Y, N=1:N)
df %>% 
  ggplot(aes(x=X, y=Y, color=N)) +
  geom_point() + 
  theme_classic() -> p1

df %>% 
  ggplot(aes(x=rank(X), y=rank(Y), color=N)) +
  geom_point() + 
  theme_classic() -> p2

grid.arrange(p1, p2)
```
### Different variance
```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=2}
N <- 1000

m <- -0.01
b <- 0
X <- 1:N*m + b + rnorm(N, mean=0, sd=3)

m <- -0.08
b <- 0
Y <- 1:N*m + b + rnorm(N, mean=0, sd=1)

df <- data.frame(X=X, Y=Y, N=1:N)
df %>% 
  ggplot(aes(x=X, y=Y, color=N)) +
  geom_point() + 
  theme_classic() -> p1

df %>% 
  ggplot(aes(x=rank(X), y=rank(Y), color=N)) +
  geom_point() + 
  theme_classic() -> p2

grid.arrange(p1, p2)
```
### Change in variance
```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=2}
N <- 1000

m <- 0.01
b <- 0
X <- 1:N*m + b + rnorm(N, mean=0, sd=.5) 

m <- 0.08
b <- 0
Y <- 1:N*m + b 
Y[1:500] <- Y[1:500] + rnorm(500, mean=0, sd=.5)
Y[501:1000] <- Y[501:1000] + rnorm(500, mean=0, sd=20)

df <- data.frame(X=X, Y=Y, N=1:N)
df %>% 
  ggplot(aes(x=X, y=Y, color=N)) +
  geom_point() + 
  theme_classic() -> p1

df %>% 
  ggplot(aes(x=rank(X), y=rank(Y), color=N)) +
  geom_point() + 
  theme_classic() -> p2

grid.arrange(p1, p2)
```

### Different distributions
```{r, message=FALSE, echo=FALSE, fig.width=1.6, fig.height=2}
N <- 1000

m <- 0.01
b <- 0
X <- 1:N*m + b + rnorm(N, mean=0, sd=1)

m <- 0.02
b <- 0.1
Y <- 1:N*m + b + rlnorm(N, meanlog = 0, sdlog = 1) 

df <- data.frame(X=X, Y=Y, N=1:N)
df %>% 
  ggplot(aes(x=X, y=Y, color=N)) +
  geom_point() + 
  theme_classic() -> p1

df %>% 
  ggplot(aes(x=rank(X), y=rank(Y), color=N)) +
  geom_point() + 
  theme_classic() -> p2

grid.arrange(p1, p2)
```